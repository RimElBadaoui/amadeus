{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amadeus Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Background\n",
    "\n",
    "In this project, we will attempt to discover the features behind the popular music of each generation. For instance, if Britney Spears, “Oops I Did It Again” made the charts in 2001, and The Beatles’ “Real Love” made the charts in 1996, we want to see what made the music popular back then – was it the timbre, audio quality, or lyrics? We will then attempt to build a model that is able to predict whether a song will be popular or not, and will also attempt to apply it to modern music. \n",
    "\n",
    "Questions: \n",
    "1. What features best predict the popularity of a song?\n",
    "2. Do those features change with time?\n",
    "3. Do different features change how long it takes for a song to become popular?\n",
    "\n",
    "This is an interesting problem because it can be used to create music which is more likely to become popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources you Intend to Use?\n",
    "\n",
    "\n",
    "The Dataset that we are using is the Million Song Dataset, which can be found here: http://labrosa.ee.columbia.edu/millionsong/. \n",
    "\n",
    "We are initially using the subset (10,000 songs) of the entire dataset, and once we are confident we have a substantial model, we will then expand the database and include all million songs, while running our model on an EC2 server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Joining/Cleaning You Did (4 points)\n",
    "If data is being joined, describe the joining process and any problems with it - explain the metric used for fuzzy joins.\n",
    "\n",
    "Explain how you will handle missing or duplicate keys. Describe the tools you used to examine/repair/clean the data.\n",
    "\n",
    "If you found any statistical anomalies last time, explain how you plan to deal with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Approach (3 points)\n",
    "Describe what analysis you are doing: This will probably comprise:\n",
    "\n",
    "- Featurization: Explain how you generated features from the raw data. e.g. thresholding to produce binary features, binning, tf-idf, multinomial -> multiple binary features (one-hot encoding). \n",
    "- Describe any value transformations you did, e.g. histogram normalization.\n",
    "- Modeling: Which machine learning models did you try? Which do you plan to try in the future?\n",
    "- Performance measurement: How will you evaluate your model and improve featurization etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results (6 Points)\n",
    "Summarize the results you have so far:\n",
    "\n",
    "Define suitable performance measures for your problem. Explain why they make sense, and what other measures you considered.\n",
    "Give the results. These might include accuracy scores, ROC plots and AUC, or precision/recall plots, or results of hypothesis tests.\n",
    "Describe any tuning that you did.\n",
    "Explain any hypothesis tests you did. Be explicit about the null and alternative hypothesis. Be very clear about the test you used and how you used it. Include all the experiment details (between/within-subjects, degrees-of-freedom etc). Be frugal with tests. Do not try many tests and report the best results.\n",
    "Use graphics! Please use visual presentation whenever possible. The next best option is a table. Try to avoid \"inlining\" important results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from setup import *\n",
    "from sklearn import datasets\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method to convert all hdf5 files into csv with 10,000 lines of format:\n",
    "data:\n",
    "key, mode, tempo, time_signature, loudness, *timbre*\n",
    "\n",
    "target:\n",
    "year\n",
    "\n",
    "The indices of the two match up.\n",
    "\n",
    "\"\"\"\n",
    "def convert_to_csv():\n",
    "    i = 0\n",
    "    data = []\n",
    "    target = []\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(msd_subset_data_path):\n",
    "        files = glob.glob(os.path.join(root,'*.h5'))\n",
    "        for f in files:\n",
    "            print(\"Getting data from: \" + str(f))\n",
    "            with parser.File(f, 'r') as h5:\n",
    "                year = get_year(h5)\n",
    "                if year:\n",
    "                    count +=1\n",
    "                    target.append([year])\n",
    "                    row = []\n",
    "                    print(\"Getting duration...\")\n",
    "                    row += [get_analysis_property(h5,'duration')]\n",
    "                    print(\"Getting End Fade...\")\n",
    "                    row += [get_analysis_property(h5,'end_of_fade_in')]\n",
    "                    print(\"Getting Key...\")\n",
    "                    row += [get_analysis_property(h5,'key')]\n",
    "                    row += [get_analysis_property(h5,'key_confidence')]\n",
    "                    print(\"Getting Loudness...\")\n",
    "                    row += [get_analysis_property(h5,'loudness')]\n",
    "                    print(\"Getting Start Fade Out...\")\n",
    "                    row += [get_analysis_property(h5,'start_of_fade_out')]\n",
    "                    print(\"Getting Tempo...\")\n",
    "                    row += [get_analysis_property(h5,'tempo')]\n",
    "                    print(\"Getting Time Signiture...\")\n",
    "                    row += [get_analysis_property(h5,'time_signature')]\n",
    "                    row += [get_analysis_property(h5,'time_signature_confidence')]\n",
    "                    print(str(len(row)) + \" Features aquired.\")\n",
    "                    print(row)\n",
    "                    # uncomment row below to get the timbre as well.\n",
    "                    # row += [get_timbre(h5)]\n",
    "                    # print row\n",
    "                    data.append(row)\n",
    "                    i+=1\n",
    "\n",
    "    with open('data_no_timbre.csv', 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print target\n",
    "    with open('target_no_timbre.csv', 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(target)\n",
    "\n",
    "def get_timbre(h5):\n",
    "    listy = []\n",
    "    timbres = h5['analysis']['segments_timbre']\n",
    "    print len(timbres)\n",
    "    timbres = np.array(timbres, dtype='f2')\n",
    "    timbres = timbres.flatten()\n",
    "    return list(timbres)\n",
    "\n",
    "def get_analysis_property(h5, prop):\n",
    "    to_return = h5['/analysis/songs'][prop][0]\n",
    "    if to_return:\n",
    "        return to_return\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_year(h5):\n",
    "    to_return = h5['/musicbrainz/songs']['year'][0]\n",
    "    if to_return:\n",
    "        return to_return\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "convert_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis, any Obstacles (3 Points)\n",
    "Describe the final analysis you plan to do:\n",
    "\n",
    "- Scale: how much data will you use?\n",
    "- Model complexity: What complexity of models will you use, this is relevant for models like clustering, factor models, Random Forests etc.\n",
    "- What tools will you use?\n",
    "- Estimate of processing time? You should be able to form an estimate of how much time you need on your chosen tools.\n",
    "and outline any obstacles you foresee.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
