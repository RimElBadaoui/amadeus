{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amadeus Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Background\n",
    "\n",
    "In this project, we will attempt to discover the features behind the popular music of each generation. For instance, if Britney Spears, “Oops I Did It Again” made the charts in 2001, and The Beatles’ “Real Love” made the charts in 1996, we want to see what made the music popular back then – was it the timbre, audio quality, or lyrics? We will then attempt to build a model that is able to predict whether a song will be popular or not, and will also attempt to apply it to modern music. \n",
    "\n",
    "Questions: \n",
    "1. What features best predict the popularity of a song?\n",
    "2. Do those features change with time?\n",
    "3. Do different features change how long it takes for a song to become popular?\n",
    "\n",
    "This is an interesting problem because it can be used to create music which is more likely to become popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources you Intend to Use?\n",
    "\n",
    "\n",
    "The Dataset that we are using is the Million Song Dataset, which can be found here: http://labrosa.ee.columbia.edu/millionsong/. \n",
    "\n",
    "We are initially using the subset (10,000 songs) of the entire dataset, and once we are confident we have a substantial model, we will then expand the database and include all million songs, while running our model on an EC2 server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Joining/Cleaning You Did (4 points)\n",
    "If data is being joined, describe the joining process and any problems with it - explain the metric used for fuzzy joins.\n",
    "\n",
    "Explain how you will handle missing or duplicate keys. Describe the tools you used to examine/repair/clean the data.\n",
    "\n",
    "If you found any statistical anomalies last time, explain how you plan to deal with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started extracting features out of the dataset – this included duration, end of fade in, key, key confidence, loudness, start of fade out, tempo, time signature, and time signature confidence. These were all features that were already in a float format, so it was easy to extract them.\n",
    "\n",
    "For missing keys, (e.g. NaNs), we removed them as part of our model prediction, since neutral values were not valid. There is no duplicate data (as promised by the data source), but we will do fuzzy joins to try to match potentially mislabelled data in the dataset to real world data.\n",
    "Currently, the data is denormalized into a HDF5 data model for every song. In order to process them, we will have to enumerate through each file to find out more about its characteristics. However, for more generic ones like all the names of the artistes, we will be using the databases that are provided in the .db format. We will then extract more information by performing an equi-join on the idx_artist_id field (which is a primary key in the subset_track_metadata.db file, and a foreign key in most of the other databases). To do this, we use the sqlite3 library in Python which will help us run SQL queries.\n",
    "\n",
    "We used h5py to help parse, scipy to collect statistics and the csv library to help print to csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Approach (3 points)\n",
    "Describe what analysis you are doing: This will probably comprise:\n",
    "\n",
    "- Featurization: Explain how you generated features from the raw data. e.g. thresholding to produce binary features, binning, tf-idf, multinomial -> multiple binary features (one-hot encoding). \n",
    "- Describe any value transformations you did, e.g. histogram normalization.\n",
    "- Modeling: Which machine learning models did you try? Which do you plan to try in the future?\n",
    "- Performance measurement: How will you evaluate your model and improve featurization etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization\n",
    "\n",
    "Our featurization occurs in the script below. We first need to extract a set of important features from the hd5 dataformat, we chose duration, key, loudness, end of fade in, start of fade out, tempo, and time signiture. Most of our features from didn't require additional cleaning, as many are already in workable formats (float) we simply piped them into our various models. We ensured that at least 90% of the rows were filled, before extracting them to form part of our model. For one particular feature, ‘timbre’, it was stored as a 2d array which was of varying sizes. To tackle this, we converted it to a 1D array, and then ensured that they were all of the same size.\n",
    "\n",
    "\n",
    "### Modelling\n",
    "\n",
    "We tried a number of machine learning models: k Nearest Neighbors, Random Forest, Linear Regression, and Logistic Regression. We had relatively poor performance with k Nearest Neighbors, and slightly better performance with Random Tree, Logistic Regression, and Linear Regression. We plan on further tuning the above models to improve performance, and additionally trying unsupervised machine learning to attempt to cluster our songs on measures besides year released (potential clusters could be genre, band gender, band age). For validation of these measures we'll also need to acquire a dataset which contains information about the bands in our dataset. \n",
    "\n",
    "### Performance Measurements\n",
    "\n",
    "For our preliminary analysis, we aimed for completing one of our initial goals of determining when a song was released based on a song's features. Since our song release year range is about 100 years, we considered a successful classification as being within 5 years of the correct release year. We only considered the accuracy around this classification as being indicative of our model's performance. We used cross validation to help us measure our performance. By holding out 10% as the test set, we were easily able to test our model against our own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(148.03546, 'duration'), (0.14799999999999999, 'end_of_fade_in'), (6, 'key'), (0.16900000000000001, 'key_confidence'), (-9.843, 'loudness'), (137.91499999999999, 'start_of_fade_out'), (121.274, 'tempo'), (4, 'time_signature'), (0.38400000000000001, 'time_signature_confidence')]\n"
     ]
    }
   ],
   "source": [
    "from setup import *\n",
    "from sklearn import datasets\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "print_local = False # Allows output supression in ipynotebook. \n",
    "\n",
    "\"\"\"\n",
    "Method to convert all hdf5 files into csv with 10,000 lines of format:\n",
    "data:\n",
    "key, mode, tempo, time_signature, loudness, *timbre*\n",
    "\n",
    "target:\n",
    "year\n",
    "\n",
    "The indices of the two match up.\n",
    "\n",
    "\"\"\"\n",
    "def convert_to_csv():\n",
    "    i = 0\n",
    "    header = ['duration',\n",
    "            'end_of_fade_in',\n",
    "            'key',\n",
    "            'key_confidence',\n",
    "            'loudness',\n",
    "            'start_of_fade_out',\n",
    "            'tempo',\n",
    "            'time_signature',\n",
    "            'time_signature_confidence']\n",
    "    #Include header which describes features extracted.\n",
    "    data = [header]\n",
    "    target = []\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(msd_subset_data_path):\n",
    "        files = glob.glob(os.path.join(root,'*.h5'))\n",
    "        for f in files:\n",
    "            local_print(\"Getting data from: \" + str(f))\n",
    "            with parser.File(f, 'r') as h5:\n",
    "                year = get_year(h5)\n",
    "                if year:\n",
    "                    count +=1\n",
    "                    target.append([year])\n",
    "                    row = []\n",
    "                    local_print(\"Getting duration...\")\n",
    "                    row += [get_analysis_property(h5,'duration')]\n",
    "                    local_print(\"Getting End Fade...\")\n",
    "                    row += [get_analysis_property(h5,'end_of_fade_in')]\n",
    "                    local_print(\"Getting Key...\")\n",
    "                    row += [get_analysis_property(h5,'key')]\n",
    "                    row += [get_analysis_property(h5,'key_confidence')]\n",
    "                    local_print(\"Getting Loudness...\")\n",
    "                    row += [get_analysis_property(h5,'loudness')]\n",
    "                    local_print(\"Getting Start Fade Out...\")\n",
    "                    row += [get_analysis_property(h5,'start_of_fade_out')]\n",
    "                    local_print(\"Getting Tempo...\")\n",
    "                    row += [get_analysis_property(h5,'tempo')]\n",
    "                    local_print(\"Getting Time Signiture...\")\n",
    "                    row += [get_analysis_property(h5,'time_signature')]\n",
    "                    row += [get_analysis_property(h5,'time_signature_confidence')]\n",
    "                    local_print(str(len(row)) + \" Features aquired.\")\n",
    "                    local_print(row)\n",
    "                    # uncomment row below to get the timbre as well.\n",
    "                    # row += [get_timbre(h5)]\n",
    "                    # print row\n",
    "                    data.append(row)\n",
    "                    i+=1\n",
    "\n",
    "    with open('data_no_timbre.csv', 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    local_print(target)\n",
    "    with open('target_no_timbre.csv', 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(target)\n",
    "        \n",
    "    sample = data[1]\n",
    "    return zip(sample, header)\n",
    "\n",
    "def local_print(string):\n",
    "    if print_local:\n",
    "        print(string)\n",
    "\n",
    "def get_timbre(h5):\n",
    "    listy = []\n",
    "    timbres = h5['analysis']['segments_timbre']\n",
    "    print_local(len(timbres))\n",
    "    timbres = np.array(timbres, dtype='f2')\n",
    "    timbres = timbres.flatten()\n",
    "    return list(timbres)\n",
    "\n",
    "def get_analysis_property(h5, prop):\n",
    "    to_return = h5['/analysis/songs'][prop][0]\n",
    "    if to_return:\n",
    "        return to_return\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_year(h5):\n",
    "    to_return = h5['/musicbrainz/songs']['year'][0]\n",
    "    if to_return:\n",
    "        return to_return\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sample = convert_to_csv()\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results (6 Points)\n",
    "Summarize the results you have so far:\n",
    "\n",
    "Define suitable performance measures for your problem. Explain why they make sense, and what other measures you considered.\n",
    "Give the results. These might include accuracy scores, ROC plots and AUC, or precision/recall plots, or results of hypothesis tests.\n",
    "Describe any tuning that you did.\n",
    "Explain any hypothesis tests you did. Be explicit about the null and alternative hypothesis. Be very clear about the test you used and how you used it. Include all the experiment details (between/within-subjects, degrees-of-freedom etc). Be frugal with tests. Do not try many tests and report the best results.\n",
    "Use graphics! Please use visual presentation whenever possible. The next best option is a table. Try to avoid \"inlining\" important results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are the results of our Machine Learning experiments on our data. The script which generated this output is below for reference:\n",
    "\n",
    "\n",
    "| Model               | Prediction Accuracy on Test Set |\n",
    "|---------------------|---------------------------------|\n",
    "| k Nearest Neighbors | 25.4%                           |\n",
    "| Neural Networks     | 25.0%                           |\n",
    "| Linear Regression   | 44.9%                           |\n",
    "| Logistic Regression | 45.2%                           |\n",
    "| Random Forest       | 44.0%                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Creating test and train data...\n",
      "Generating kNN...\n",
      "Generating Random Forest...\n",
      "Generating Linear Regression...\n",
      "Generating Logistic Regression...\n",
      "kNN Accuracy: \n",
      "Number of close guesses are: 119\n",
      "Total accuracy = 0.254273504274\n",
      "Random Forest Accuracy: \n",
      "Number of close guesses are: 192\n",
      "Total accuracy = 0.410256410256\n",
      "Linear Regression Accuracy: \n",
      "Number of close guesses are: 193\n",
      "Total accuracy = 0.412393162393\n",
      "Logistic Regression Accuracy: \n",
      "Number of close guesses are: 218\n",
      "Total accuracy = 0.465811965812\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation, ensemble, linear_model, feature_selection, neighbors\n",
    "import csv\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Data...\")\n",
    "    with open('data_no_timbre.csv','r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "    train = np.genfromtxt(open('data_no_timbre.csv','r'), delimiter=',', dtype='f8', skip_header=1)\n",
    "    train[np.isnan(train)] = 0\n",
    "    target = np.genfromtxt(open('target_no_timbre.csv','r'), delimiter=',', dtype='f8')\n",
    "\n",
    "    print(\"Creating test and train data...\")\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(train, target, test_size=0.1, random_state=0)\n",
    "\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    # kNN classifier\n",
    "    print(\"Generating kNN...\")\n",
    "    knn = neighbors.KNeighborsClassifier() # default neighbors is 5,\n",
    "    knn.fit(X_train, y_train);\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"Generating Random Forest...\")\n",
    "    rf = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=8)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Linear Regression Model\n",
    "    print(\"Generating Linear Regression...\")\n",
    "    lr = linear_model.LinearRegression(n_jobs=8)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Logistic Regression Model\n",
    "    print(\"Generating Logistic Regression...\")\n",
    "    lr2 = linear_model.LogisticRegression()\n",
    "    # lr2.fit(X_train, y_train)\n",
    "\n",
    "    rfe = feature_selection.RFE(lr2, 2)\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    print(\"kNN Accuracy: \")\n",
    "    model_accuracy(knn, X_test, y_test)\n",
    "    print(\"Random Forest Accuracy: \")\n",
    "    model_accuracy(rf, X_test, y_test)\n",
    "    print(\"Linear Regression Accuracy: \")\n",
    "    model_accuracy(lr, X_test, y_test)\n",
    "    print(\"Logistic Regression Accuracy: \")\n",
    "    model_accuracy(rfe, X_test, y_test)\n",
    "\n",
    "def model_accuracy(model, test_set, test_target):\n",
    "    guesses = model.predict(test_set)\n",
    "    right = 0\n",
    "\n",
    "    for counter, guess in enumerate(guesses):\n",
    "        if abs(guess - test_target[counter]) < 5:\n",
    "            right += 1\n",
    "\n",
    "    print 'Number of close guesses are: ' + str(right)\n",
    "    print 'Total accuracy = ' + str(right*1.0/len(test_target))\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis, any Obstacles (3 Points)\n",
    "Describe the final analysis you plan to do:\n",
    "\n",
    "- Scale: how much data will you use?\n",
    "- Model complexity: What complexity of models will you use, this is relevant for models like clustering, factor models, Random Forests etc.\n",
    "- What tools will you use?\n",
    "- Estimate of processing time? You should be able to form an estimate of how much time you need on your chosen tools.\n",
    "and outline any obstacles you foresee.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scale\n",
    "\n",
    "We've been running our preliminary analysis on a subset of 10,000 songs (2.7 GB). We will be running our final analysis on 1,000,000 songs (~270GB). \n",
    "\n",
    "### Model Complexity\n",
    "\n",
    "We've used fairly basic configurations for our Random Forest and other ML algorithms, as we continue to explore our models, we'll be refining our models to try to improve our predictions. \n",
    "\n",
    "### Tools\n",
    "\n",
    "The tools we will be using will be mainly **scikitlearn** for our machine learning and modelling needs. We'll also likely use **d3** and **matplotlib** to create visualizations of the factors that we are considering. \n",
    "\n",
    "### Estimate of Processing Time\n",
    "\n",
    "The above processing takes about 5 minutes to convert our hd5 data into a useable format, we expect that to scale to around 9 hours when we scale to the full dataset. The model generation time will likely also increase proportionately, from 2 minutes to around 3.5 hours. Our total processing time will likely be half a day for the 270 GB of data we expect to process. We will be performing this full analysis on an EC2 instance provided by the course staff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
